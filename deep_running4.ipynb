{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_running4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/15108326/ANAE/blob/master/deep_running4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spg8BLDO8xGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer=load_breast_cancer()\n",
        "x=cancer.data\n",
        "y=cancer.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYDOm8M1b50e",
        "colab_type": "text"
      },
      "source": [
        "#**04-5 로지스틱회귀를 위한 뉴런을 만듭니다.**\n",
        "3장에서 훈련 데이터 세트 전체를 사용하여 모델을 훈련했습니다. 그러나 모델의 성능을 평가하지 않았고 따라서 모델이 맞는지 아닌지를 알수가 없습니다. 여기서는 모델의 성능을 평가하는 방법에 대해서 알아보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM9PndKwgaFO",
        "colab_type": "text"
      },
      "source": [
        "##모델의 성능 평가를 위한 훈련 세트와 테스트 세트 \n",
        "훈련된 모델의 실전 성능을 일반화 성능(generalization performance)이라고 부릅니다. 그런데 모델에 학습시킨 훈련 데이터 세트로 다시 모델의 성능을 평가하면 그 모델은 당연히 좋은 성능이 나올 것입니다. 이런 성능 평가를 '과도하게 낙관적으로 일반화 성능을 추정한다'고 말합니다. 따라서 훈련 데이터 세트를 두 덩어리로 나누어 하나는 훈련에, 다른 하나는 테스트에 사용하면 됩니다.  \n",
        "\n",
        "**그림 추가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUcCAlITht0q",
        "colab_type": "text"
      },
      "source": [
        "##훈련 세트와 테스트 세트로 나누기\n",
        " \n",
        "* 훈련 데이트 세트를 나눌 때는 테스트 \n",
        "세트보다 훈련 세트가 더 많아야 합니다.  \n",
        "* 훈련 데이터 세트를 나눌 때는 양성, 음성 클래스가 훈련 세트와 테스트 세트에 고르게 분포하도록 만들어야 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmBCF0JRjv-0",
        "colab_type": "text"
      },
      "source": [
        "만약 훈련 세트에 양성 클래스가 많이 몰리거나 테스트 세트에 음성 클래스가 너무 많이 몰리면 모델이 데이터에 있는 패턴을 올바르게 학습하지 못하거나 성능을 잘못 측정할 수도 있습니다.\n",
        "\n",
        "**그림추가**\n",
        "\n",
        "위 그림을 보면 위쪽의 경우에는 그대로 2대1로 나누어져 잘 나누어졌고 밑쪽의 경우에는 클래스 비율이 망가져 있습니다. 클래스의 비율이 망가지면 모델에 사용할 데이터가 올바르지 않으므로 결과도 좋지 않습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrkq2juuruWZ",
        "colab_type": "text"
      },
      "source": [
        "###1. train_test_split() 함수로 훈련 데이터 세트 나누기\n",
        "사이킷런의 train_test_split() 함수는 기본적으로 입력된 훈련 데이터 세트를 훈련 세트 75%,테스트 세트 25% 비율로 나눠줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gh9p1uNib1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split # sklearn.model_selection 모듈에서 train_test_split()함수를 임포트"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsA0M79-tF7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,stratify=y,test_size=0.2,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_BcoAXKvS5e",
        "colab_type": "text"
      },
      "source": [
        "* stratify는 훈련 데이터를 나눌 때 클래스 비율을 동일하게 만듭니다.\n",
        "* test_size는 기본적인 훈련 데이터 세트를 나누는 비율을 조절하고 싶을 때 사용합니다. 여기서는 test_size에 0.2를 전달하여 입력된 데이터 세트의 20%를 테스트 세트로 나눴습니다.\n",
        "* 무작위로 섞은 결과를 항상 일정하도록 하기 위해서 random_state에 난수 초깃값을 지정했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6EIaeeMw0I6",
        "colab_type": "text"
      },
      "source": [
        "###2. 결과 확인하기\n",
        "이제 훈련 데이터 세트가 잘 나누어졌는지 훈련 세트와 테스트 세트의 비율을 확인해 보겠습니다. shape속성을 이용해 확인해 보면 훈련 세트와 테스트 세트가 4:1의 비율로 잘 나누어진 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue76DwHHt9rL",
        "colab_type": "code",
        "outputId": "112d508c-4bf0-498e-a734-90547b4c4eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_train.shape,x_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(455, 30) (114, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNOubFC089d",
        "colab_type": "text"
      },
      "source": [
        "###3. unique() 함수로 훈련 세트의 타깃 확인하기\n",
        "넘파이의 unique() 함수로 훈련 세트의 타깃안에 있는 클래스의 개수를 확인해 보면 전체 훈련 데이터 세트의 클래스 비율과 거의 비슷한 구성인것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Korbk4Yp2fyv",
        "colab_type": "code",
        "outputId": "b6572dfd-fb62-48e9-f4e6-94017469ba69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique(y_train, return_counts=True) # y의 요소를 중복없이 알려주며 return_counts를 통해서 각각의 요소의 수를 알 수 있습니다."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([170, 285]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vDZSYm154Hj",
        "colab_type": "text"
      },
      "source": [
        "##로지스틱 회귀 구현하기\n",
        "준비된 훈련 세트를 바탕으로 로지스틱 회귀를 구현해 보겠습니다. 로지스틱 회귀를 정방향으로 데이터가 흘러가는 과정(정방향 계산)부터 가중치를 업데이트하기 위해 역방향으로 데이터가 흘러가는 과정(역방향 계산)까지 순서대로 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WyAsuVI6kXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticNeuron:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def forpass(self, x):\n",
        "        z = np.sum(x * self.w) + self.b  # 직선 방정식을 계산합니다\n",
        "        return z\n",
        "\n",
        "    def backprop(self, x, err):\n",
        "        w_grad = x * err    # 가중치에 대한 그래디언트를 계산합니다\n",
        "        b_grad = 1 * err    # 절편에 대한 그래디언트를 계산합니다\n",
        "        return w_grad, b_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc09lot460VE",
        "colab_type": "text"
      },
      "source": [
        "보면 3장에서 구현했던 Neuron클래스와는 다소 차이점이 있습니다. \n",
        "* 입력 데이터의 특성이 많아 `__init__()` 메서드는 가중치와 절편을 미리 초기화 하지 않습니다. 나중에 입력 데이터를 보고 특성 개수에 맞게 결정합니다.\n",
        "* forpass() 메서드를 보면 가중치와 입력 특성의 곱을 모두 더하기 위해 np.sum()함수를 사용한 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J4LXoWn8YyG",
        "colab_type": "text"
      },
      "source": [
        "##훈련하는 메서드 구현하기\n",
        "###1. fit() 메서드 구현하기\n",
        "훈련을 수행하는 fit() 메서드를 구현해 보겠습니다. 기본 구조는 3장의 Neuron클래스와 같지만 활성화 함수가 추가되었고 역방향 계산에는 로지스틱 손실 함수의 도함수를 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj4Mz3j79GWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(self, x, y, epochs=100):\n",
        "    self.w = np.ones(x.shape[1])      # 가중치를 초기화합니다.\n",
        "    self.b = 0                        # 절편을 초기화합니다.\n",
        "    for i in range(epochs):           # epochs만큼 반복합니다\n",
        "        for x_i, y_i in zip(x, y):    # 모든 샘플에 대해 반복합니다 , zip을 이용해서 자료를 묶어줍니다.\n",
        "            z = self.forpass(x_i)     # 정방향 계산\n",
        "            a = self.activation(z)    # 활성화 함수 적용\n",
        "            err = -(y_i - a)          # 오차 계산\n",
        "            w_grad, b_grad = self.backprop(x_i, err) # 역방향 계산\n",
        "            self.w -= w_grad          # 가중치 업데이트\n",
        "            self.b -= b_grad          # 절편 업데이트"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgDw9TQW_9Zd",
        "colab_type": "text"
      },
      "source": [
        "###2. activation() 메서드 구현하기\n",
        "시그모이드 함수가 사용되어야 합니다. 시그모이드 함수는 넘파이의 np.exp() 함수를 사용하여 간단히 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaSCdh5V_1qB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(self, z):\n",
        "    a = 1 / (1 + np.exp(-z))  # 시그모이드 계산\n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJyXzC5vAcjs",
        "colab_type": "text"
      },
      "source": [
        "##예측하는 메서드 구현하기\n",
        "3장에서는 forpass() 메서드를 사용하여 새로운 샘플에 대한 예측값을 계산했으나 여러개의 샘플을 한꺼번에 예측할 때는 여러 번 호출해야하는 번거로움이 있습니다. 또한 분류에서는 활성화 함수와 임계 함수도 적용해야 하므로 새롭게 predict() 메서드를 만들어보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDVUgyD1BFHT",
        "colab_type": "text"
      },
      "source": [
        "###1. predict() 메서드 구현하기\n",
        "predict() 메서드의 매개변수 값으로 입력값 x가 2차원 배열로 전달된다고 가정하고 구현하게습니다. 예측값은 입력값을 선형 함수, 활성화 함수, 임계 함수 순서로 통과시키면 구할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KAb06v0AZCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(self, x):\n",
        "    z = [self.forpass(x_i) for x_i in x]    # 정방향 계산\n",
        "    a = self.activation(np.array(z))        # 활성화 함수 적용\n",
        "    return a > 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlsgtzLBCFmL",
        "colab_type": "text"
      },
      "source": [
        "여기서 z의 계산에서 파이썬의 리스트 내포 문법을 사용했습니다. 리스트 내포란 대괄호([]) 안에 for문을 삽입하여 새 리스트를 만드는 문법입니다. x의 행을 하나씩 꺼내어 forpass() 메서드에 적용하고 그 결과를 이용하여 새 리스트(z)로 만드는 것입니다. z는 곧바로 넘파이 배열로 바꾸어 activation() 메서드로 전달됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5yOivQACW4j",
        "colab_type": "text"
      },
      "source": [
        "##구현 내용 한눈에 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpuqUe4GCD6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticNeuron:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def forpass(self, x):\n",
        "        z = np.sum(x * self.w) + self.b  # 직선 방정식을 계산합니다\n",
        "        return z\n",
        "\n",
        "    def backprop(self, x, err):\n",
        "        w_grad = x * err    # 가중치에 대한 그래디언트를 계산합니다\n",
        "        b_grad = 1 * err    # 절편에 대한 그래디언트를 계산합니다\n",
        "        return w_grad, b_grad\n",
        "\n",
        "    def activation(self, z):\n",
        "        a = 1 / (1 + np.exp(-z))  # 시그모이드 계산\n",
        "        return a\n",
        "        \n",
        "    def fit(self, x, y, epochs=100):\n",
        "        self.w = np.ones(x.shape[1])      # 가중치를 초기화합니다.\n",
        "        self.b = 0                        # 절편을 초기화합니다.\n",
        "        for i in range(epochs):           # epochs만큼 반복합니다\n",
        "            for x_i, y_i in zip(x, y):    # 모든 샘플에 대해 반복합니다\n",
        "                z = self.forpass(x_i)     # 정방향 계산\n",
        "                a = self.activation(z)    # 활성화 함수 적용\n",
        "                err = -(y_i - a)          # 오차 계산\n",
        "                w_grad, b_grad = self.backprop(x_i, err) # 역방향 계산\n",
        "                self.w -= w_grad          # 가중치 업데이트\n",
        "                self.b -= b_grad          # 절편 업데이트\n",
        "    \n",
        "    def predict(self, x):\n",
        "        z = [self.forpass(x_i) for x_i in x]    # 정방향 계산\n",
        "        a = self.activation(np.array(z))        # 활성화 함수 적용\n",
        "        return a > 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7oVn4-2D0cE",
        "colab_type": "text"
      },
      "source": [
        "##로지스틱 회귀 모델 훈련시키기\n",
        "준비한 데이터 세트를 바탕으로 로지스틱 회귀 모델을 훈련하고 정확도를 측정하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO9eQ8jREqn1",
        "colab_type": "text"
      },
      "source": [
        "###1.모델 훈련하기\n",
        "클래스의 객체를 만들고 훈련 세트와 함께 fit() 메서드를 호출하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVMpnH34EYiY",
        "colab_type": "code",
        "outputId": "a4efd766-9ba0-4971-adfc-19f5b24a6240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neuron = LogisticNeuron()\n",
        "neuron.fit(x_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE5GzkEVHFPm",
        "colab_type": "text"
      },
      "source": [
        "##2. 테스트 세트 사용해 모델의 정확도 평가하기\n",
        "훈련이 끝난 모델에 테스트 세트를 사용해 예측값을 넣고 예측한 값이 맞는지 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHjAQSzGF0Aj",
        "colab_type": "code",
        "outputId": "2dc0f815-dc7e-4a5d-f139-5d85c9b7ed7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "np.mean(neuron.predict(x_test) == y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8245614035087719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jlcLYZ7H_mW",
        "colab_type": "text"
      },
      "source": [
        "predict() 메서드의 반환값은 True나 False로 채워진 (m,) 크기의 배열이고 y_test는 0 또는 1로 채워진 (m,)크기의 배열이므로 바로 비교할 수 있습니다.np.mean() 함수는 매개변수 값으로 전달한 비교문 결과의 평균을 계산합니다. 즉, 계산 결과 0.82는 올바르게 예측한 샘플의 비율이 됩니다. 이를 정확도(accuracy)라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGos9o5zIYT2",
        "colab_type": "text"
      },
      "source": [
        "#**04-6 로지스틱 회귀 뉴런으로 단일층 신경망을 만듭니다.**\n",
        "## 일반적인 신경망의 모습을 알아봅니다.\n",
        "일반적으로 신경망은 다음과 같이 표현합니다. 가장 왼쪽이 입력층(input layer), 가장 오른쪽이 출력층(output layer) 그리고 가운데 층들을 은닉층(hidden layer)이라고 부릅니다. 작은 원으로 표시된 활성화 함수는 각 층의 한 부분으로 간주합니다.\n",
        "\n",
        "**그림 추가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeKor1upJHEv",
        "colab_type": "text"
      },
      "source": [
        "###단일층 신경망의 모습을 알아봅니다.\n",
        "앞에서 배운 로지스틱 회귀는 은닉층이 없는 신경망이라고 볼 수 있습니다. 이런 입력층과 출력층만 가지는 신경망을 단일층 신경망이라고 부릅니다. 이거을 그림으로 나타내면 아래와 같습니다.\n",
        "\n",
        "**그림 추가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n2MPIYoJoWI",
        "colab_type": "text"
      },
      "source": [
        "##단일층 신경망을 구현합니다.\n",
        "여기서는 미리 구현한 LogisticNeuron 클래스에 몇 가지 유용한 기능을 추가하여 다시 구현해보겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaekQnfWKCcN",
        "colab_type": "text"
      },
      "source": [
        "##손실 함수의 결괏값을 조정해 저장 기능 추가하기\n",
        "`__init__()` 메서드에 손실 함수의 결괏값을 저장할 리스트 self,losses를 만듭니다.그런 다음 샘플마다 손실 함수를 계산하고 그 결괏값을 모두 더한 다음 샘플 개수로 나눈 평균값을 self.losses 변수에 저장합니다. 그리고 self.activation() 메서드로 계산한 a는 np,log()의 계산을 위해 한 번 더 조정합니다. 왜냐하면 a가 0에 가까워지면 np.log() 함수의 값은 음의 무한대가 되고 a가 1에 가까워지면 np.log() 함수의 값은 0이 되기 떄문입니다. 손실값이 무한해지면 계산을 할 수 없으므로 a의 값이 $1\\times 10^{-10}\\sim 1-1\\times 10^{-10}$ 사이가 되도록 np.clip() 함수로 조정해야 합니다. np.clip() 함수는 주어진 범위 밖의 값을 범위 양 끝의 값으로 잘라 냅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm6E0_qpGFNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def __init__(self):\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "    self.losses = []\n",
        "\n",
        "def fit(self, x, y, epochs=100):\n",
        "        for i in index:                      # 모든 샘플에 대해 반복합니다\n",
        "            z = self.forpass(x[i])             # 정방향 계산\n",
        "            a = self.activation(z)             # 활성화 함수 적용\n",
        "            err = -(y[i] - a)                  # 오차 계산\n",
        "            w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산\n",
        "            self.w -= w_grad                   # 가중치 업데이트\n",
        "            self.b -= b_grad                   # 절편 업데이트\n",
        "            # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적합니다\n",
        "            a = np.clip(a, 1e-10, 1-1e-10)\n",
        "            loss += -(y[i]*np.log(a)+(1-y[i])*np.log(1-a)) # 에포크마다 평균 손실을 저장합니다\n",
        "        \n",
        "        self.losses.append(loss/len(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS0Ve1puOp-5",
        "colab_type": "text"
      },
      "source": [
        "##여러 가지 경사 하강법에 대해 알아봅니다.\n",
        "지금까지 사용한 경사 하강법은 샘플 데이터 1개에 대한 그레디언트를 계산했습니다. 이를 확률적 경사 하강법(stochastic gradient descent)이라고 부릅니다.\n",
        "\n",
        "**그림 추가**\n",
        "\n",
        "또한 확률적 경사 하강법 외에 전체 훈련 세트를 사용하여 한 번에 그레디언트를 계산하는 방식인 배치 경사 하강법(batch gradient descent)과 배치(batch) 크기를 작게 하여(훈련 세트를 여러 번 나누어) 처리하는 방식인 미니 배치 경사 하강법(mini-batch gradient descent)이 있습니다.\n",
        "\n",
        "**그림 추가**\n",
        "\n",
        "**그림 추가**\n",
        "\n",
        "확률적 경사 하강법은 샘플 데이터 1개마다 그레디언트를 계산하여 가중치를 업데이트하므로 계산 비용은 적은 대신 가중치가 최적값에 수렴하는 과정이 불안정합니다. 반면에 배치 경사 하강법은 전체 훈련 데이터 세트를 사용하여 한 번에 그레디언트를 계산하므로 가중치가 최적값에 수렴하는 과정은 안정적이지만 그만큼 계산 비용이 많이 듭니다. 이 둘의 장점을 절충한 것이 미니 배치 경사 하강법입니다. 다음은 확률적 경사 하강법, 배치 경사 하강법이 최적의 가중치(w1,w2)에 수렴하는 과정을 나타낸 그래프입니다. 미니 배치 경사 하강법은 확률적 경사 하강법보다는 매끄럽고 배치 경사 하강법보다는 덜 매끄러운 그래프가 그려집니다.\n",
        "\n",
        "**그림 추가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec-0y5KFROJO",
        "colab_type": "text"
      },
      "source": [
        "###매 에포크마다 훈련 세트의 샘플 순서를 섞어 사용하기\n",
        "훈련 세트의 샘플 순서를 섞으면 가중치 최적값의 탐색 과정이 다양해져 가중치 최적값을 제대로 찾을 수 있습니다. 다음 그림은 첫 번째 에포크에서 사용한 샘플의 순서와 두 번째 에포크에서 사용한 샘플의 순서를 나타낸 것입니다.\n",
        "\n",
        "**그림 추가**\n",
        "\n",
        "**그림 추가**\n",
        "\n",
        "훈련 세트의 샘플 순서를 섞는 전형적인 방법은 넘파이 배열의 인덱스를 섞은 수 인덱스 순서대로 샘플을 뽑는 것입니다. 이 방법이 훈련 세트 자체를 섞는 것보다 효율적이고 빠릅니다. np,random.permutation() 함수를 사용하면 이 방법을 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quk-rBK7RLbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(self, x, y, epochs=100):\n",
        "    self.w = np.ones(x.shape[1])               # 가중치를 초기화합니다.\n",
        "    self.b = 0                                 # 절편을 초기화합니다.\n",
        "    for i in range(epochs):                    # epochs만큼 반복합니다\n",
        "        loss = 0\n",
        "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다\n",
        "        for i in indexes:                      # 모든 샘플에 대해 반복합니다\n",
        "            z = self.forpass(x[i])             # 정방향 계산\n",
        "            a = self.activation(z)             # 활성화 함수 적용\n",
        "            err = -(y[i] - a)                  # 오차 계산\n",
        "            w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산\n",
        "            self.w -= w_grad                   # 가중치 업데이트\n",
        "            self.b -= b_grad                   # 절편 업데이트\n",
        "            a = np.clip(a, 1e-10, 1-1e-10)  # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적합니다\n",
        "            loss += -(y[i]*np.log(a)+(1-y[i])*np.log(1-a))  # 에포크마다 평균 손실을 저장합니다\n",
        "        self.losses.append(loss/len(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsRs8gtUTTxv",
        "colab_type": "text"
      },
      "source": [
        "###score() 메서드 추가하기\n",
        "정확도를 계산해 주는 score() 메서드를 추가하고 predict() 메서드도 수정하겠습니다. score() 메서드에는 정확도를 계산할 때 사용했던 np.mean() 함수를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpgRxdrETrqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(self, x):\n",
        "    z = [self.forpass(x_i) for x_i in x]     # 정방향 계산\n",
        "    return np.array(z) > 0                   # 스텝 함수 적용\n",
        "    \n",
        "def score(self, x, y):\n",
        "    return np.mean(self.predict(x) == y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHUEI7GhT4in",
        "colab_type": "text"
      },
      "source": [
        "시그모이드 함수의 출력값은 0~1 사이의 확률값이고 양성 클래스를 판단하는 기준은 0.5이상입니다. 그런데 z가 0보다 크면 시그모이드 함수의 출력값은 0.5보다 크고 z가 0보다 작으면 시그모이드 함수의 출력값은 0.5보다 작습니다. 그래서 predict() 메서드에는굳이 시그모이드 함수를 사용하지 않아도 됩니다. 그래서 predict() 메서드에는 로지스틱 함수를 적용하지 않고 z값의 크기만 비교하여 결과를 반환했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye7x6yY3VV1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SingleLayer:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.losses = []\n",
        "\n",
        "    def forpass(self, x):\n",
        "        z = np.sum(x * self.w) + self.b  # 직선 방정식을 계산합니다\n",
        "        return z\n",
        "\n",
        "    def backprop(self, x, err):\n",
        "        w_grad = x * err    # 가중치에 대한 그래디언트를 계산합니다\n",
        "        b_grad = 1 * err    # 절편에 대한 그래디언트를 계산합니다\n",
        "        return w_grad, b_grad\n",
        "\n",
        "    def activation(self, z):\n",
        "        a = 1 / (1 + np.exp(-z))  # 시그모이드 계산\n",
        "        return a\n",
        "        \n",
        "    def fit(self, x, y, epochs=100):\n",
        "        self.w = np.ones(x.shape[1])               # 가중치를 초기화합니다.\n",
        "        self.b = 0                                 # 절편을 초기화합니다.\n",
        "        for i in range(epochs):                    # epochs만큼 반복합니다\n",
        "            loss = 0\n",
        "            indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다\n",
        "            for i in indexes:                      # 모든 샘플에 대해 반복합니다\n",
        "                z = self.forpass(x[i])             # 정방향 계산\n",
        "                a = self.activation(z)             # 활성화 함수 적용\n",
        "                err = -(y[i] - a)                  # 오차 계산\n",
        "                w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산\n",
        "                self.w -= w_grad                   # 가중치 업데이트\n",
        "                self.b -= b_grad                   # 절편 업데이트\n",
        "                a = np.clip(a, 1e-10, 1-1e-10)     # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적합니다\n",
        "                loss += -(y[i]*np.log(a)+(1-y[i])*np.log(1-a))  # 에포크마다 평균 손실을 저장합니다\n",
        "            self.losses.append(loss/len(y))\n",
        "    \n",
        "    def predict(self, x):\n",
        "        z = [self.forpass(x_i) for x_i in x]     # 정방향 계산\n",
        "        return np.array(z) > 0                   # 스텝 함수 적용\n",
        "    \n",
        "    def score(self, x, y):\n",
        "        return np.mean(self.predict(x) == y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekY-sD5kWO_w",
        "colab_type": "text"
      },
      "source": [
        "##단일층 신경망 훈련하기\n",
        "###1. 단일층 신경망 훈련하고 정확도 출력하기\n",
        "클래스의 객체를 만들고 훈련 세트로 신경망을 훈련한 다음 score() 메서드로 정확도를 출력해겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBexHAAZWG1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "90447664-39fa-488b-dda9-68309e7e28f9"
      },
      "source": [
        "layer = SingleLayer()\n",
        "layer.fit(x_train, y_train)\n",
        "layer.score(x_test, y_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G3RnAoqXL2x",
        "colab_type": "text"
      },
      "source": [
        "##2. 손실 함수 누적값 확인하기\n",
        "성능이 에포크마다 훈련 세트를 무작위로 섞어 손실함수의 값을 줄였기 때문에 좋아졌습니다. 이를 확인하기 위해서 그래프를 그려보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCWwzKjSWomn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c34022c2-8f4d-4b3b-e596-09691b246e4b"
      },
      "source": [
        "plt.plot(layer.losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwymKTZwYFZv",
        "colab_type": "text"
      },
      "source": [
        "로지스틱 손실 함수의 값이 에포크가 진행됨에 따라 감소하고 있음을 확인할 수 있습니다. 이로써 가장 기초적인 신경망 알고리즘이 구현되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIh5xzYGYL0H",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg9eZ0gmY2fU",
        "colab_type": "text"
      },
      "source": [
        "#**04-7 사이킷런으로 로지스틱 회귀를 수행합니다.**\n",
        "사이킷런의 경사 하강법이 구현된 SGDClassifier클래스를 이용하여 로지스틱 회귀 문제를 간단히 해결해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbZoSgJrZIgZ",
        "colab_type": "text"
      },
      "source": [
        "##사이킷런으로 경사 하강법 적용하기\n",
        "###1. 로지스틱 손실 함수 지정하기\n",
        "SGDClassifier 클래스에 로지스틱 회귀를 적용하려면 loss 매개변수에 손실 함수로 log를 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVrNABM9X72o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWDTW-82Zs9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGDClassifier(loss='log', max_iter=100, tol=1e-3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18vBzu33ZyqV",
        "colab_type": "text"
      },
      "source": [
        "max_iter를 통해 반복 횟수를 100으로 지정합니다. 반복할 때마다 로지스틱 손실 함수의 값이 tol에 지정한 값만큼 감소되지 않으면 반복이 중단되도록 설정합니다. 만약 tol의 값을 설정하지 않으면 max_iter의 값을 늘리라는 경고가 발생합니다. 이는 모델의 로지스틱 손실 함수의 값이 최적값으로 수렴할 정도로 충분한 반복 횟수를 입력했는지 사용자에게 알려주므로 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAXgMShZa6Ig",
        "colab_type": "text"
      },
      "source": [
        "###2. 사이킷런으로 훈련하고 평가하기\n",
        "사이킷런의 SGDClassifier 클래스에는 지금까지 우리가 직접 구현한 메서드가 이미 준비되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voD8ffJKZx8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "235ac0d5-fbd0-4a55-8a5b-2ad054c49ee3"
      },
      "source": [
        "sgd.fit(x_train, y_train)\n",
        "sgd.score(x_test, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oyaCBThblSj",
        "colab_type": "text"
      },
      "source": [
        "###3. 사이킷런으로 예측하기\n",
        "예측을 위한 메서드도 구현되었습니다. 주의할 점은 사이킷런은 입력 데이터로 2차원 배열만 받아들입니다.즉, 샘플 하나를 주입하더라도 2차원 배열로 만들어야 합니다. 여기서는 배열의 슬라이싱을 사용해 테스트 세트에서 10개의 샘플만 뽑아 예측을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee9QwtfxbaDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec851bcc-1400-4205-b4c8-9f14991fe017"
      },
      "source": [
        "sgd.predict(x_test[0:10])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}